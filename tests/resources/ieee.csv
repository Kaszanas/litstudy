"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Exascale Computing Trends: Adjusting to the ""New Normal""' for Computer Architecture","P. Kogge; J. Shalf",University of Notre Dame; Lawrence Berkeley National Laboratory,"Computing in Science & Engineering","4 Feb 2014","2013","15","6","16","26","We now have 20 years of data under our belt about the performance of supercomputers against at least a single floating-point benchmark from dense linear algebra. Until about 2004, a single model of parallel programming, bulk synchronous using the MPI model, was sufficient to permit translation into reasonable parallel programs for more complex applications. Starting in 2004, however, a confluence of events changed forever the architectural landscape that underpinned MPI. The first half of this article goes into the underlying reasons for these changes, and what they mean for system architectures. The second half then addresses the view going forward in terms of our standard scaling models and their profound implications for future programming and algorithm design.","1558-366X","","10.1109/MCSE.2013.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634083","Computer architecture;Market research;Transistors;Programming;Computational modeling;Memory management;Systems engineering and theory;scientific computing;exascale;HPC;computer architecture;programming models","Computer architecture;Market research;Transistors;Programming;Computational modeling;Memory management;Systems engineering and theory","application program interfaces;computer architecture;floating point arithmetic;linear algebra;mainframes;message passing;parallel machines;parallel programming","standard scaling models;system architectures;MPI model;bulk synchronous;parallel programming;linear algebra;single floating-point benchmark;supercomputer;computer architecture;exascale computing","","51","","13","","16 Oct 2013","","","IEEE","IEEE Magazines"
"European HPC Landscape","F. Berberich; J. Liebmann; J. -P. Nominé; O. Pineda; P. Segers; V. Teodor","PRACE aisbl and Jülich Supercomputing Center, Forschungszetrum Juelich GmbH; Jülich Supercomputing Center, Forschungszetrum Juelich GmbH; ETP4HPC and Commissariat à l'énergie atomique et aux énergies alternatives; PRACE aisbl and Barcelona Supercomputing Center; Grand équipement national de calcul intensif; Jülich Supercomputing Center, Forschungszetrum Juelich GmbH; another affiliation","2019 15th International Conference on eScience (eScience)","19 Mar 2020","2019","","","471","478","This paper provides an overview on the European HPC landscape supported by a survey, designed by the PRACE-5IP project, accessing more than 80 of the most influential stakeholders of HPC in Europe. It focuses on Tier-0 systems on a European level providing high-end computing and data analysis resources. The different actors are presented and their provided services are analyzed in order to identify overlaps and gaps, complementarity and opportunities for collaborations. A new pan-European HPC portal is proposed in order to get all information on one place and access the different services.","","978-1-7281-2451-3","10.1109/eScience.2019.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9041695","European;High Performance Computing;HPC;Ecosystem;Exascale;services;platform","Europe;Ecosystems;Industries;Investment;Artificial intelligence;Supercomputers;Technological innovation","data analysis;energy conservation;mainframes;parallel processing;portals;power aware computing","provided services;pan-European HPC portal;European HPC landscape;PRACE-5IP project;European level;high-end computing;data analysis resources;tier-0 systems","","","","27","","19 Mar 2020","","","IEEE","IEEE Conferences"
"Tracking Performance Portability on the Yellow Brick Road to Exascale","T. Deakin; A. Poenaru; T. Lin; S. McIntosh-Smith","University of Bristol,Department of Computer Science,UK; University of Bristol,Department of Computer Science,UK; University of Bristol,Department of Computer Science,UK; University of Bristol,Department of Computer Science,UK","2020 IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC (P3HPC)","1 Jan 2021","2020","","","1","13","With Exascale machines on our immediate horizon, there is a pressing need for applications to be made ready to best exploit these systems. However, there will be multiple paths to Exascale, with each system relying on processor and accelerator technologies from different vendors. As such, applications will be required to be portable between these different architectures, but it is also critical that they are efficient too. These double requirements for portability and efficiency begets the need for performance portability. In this study we survey the performance portability of different programming models, including the open standards OpenMP and SYCL, across the diverse landscape of Exascale and pre-Exascale processors from Intel, AMD, NVIDIA, Fujitsu, Marvell, and Amazon, together encompassing GPUs and CPUs based on both x86 and Arm architectures. We also take a historical view and analyse how performance portability has changed over the last year.","","978-1-6654-2287-1","10.1109/P3HPC51967.2020.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309052","performance portability;programming models","Graphics processing units;Kernel;Biological system modeling;Computer architecture;Bandwidth;Parallel programming;Performance evaluation","application program interfaces;coprocessors;graphics processing units;library automation;microprocessor chips;multiprocessing systems;parallel architectures;parallel processing;parallel programming;power aware computing","performance portability;different programming models;pre-Exascale processors;yellow brick road;Exascale machines;processor;accelerator technologies","","5","","18","","1 Jan 2021","","","IEEE","IEEE Conferences"
"Predicting the Energy Consumption of CUDA Kernels using SimGrid","D. Boughzala; L. Lefèvre; A. -C. Orgerie","Univ Lyon, EnsL, UCBL, CNRS, Inria, LIP; Univ Lyon, EnsL, UCBL, CNRS, Inria, LIP; CNRS, IRISA","2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)","22 Oct 2020","2020","","","191","198","Building a sustainable Exascale machine is a very promising target in High Performance Computing (HPC). To tackle the energy consumption challenge while continuing to provide tremendous performance, the HPC community have rapidly adopted GPU-based systems. Today, GPUs have became the most prevailing components in the massively parallel HPC landscape thanks to their high computational power and energy efficiency. Modeling the energy consumption of applications running on GPUs has gained a lot of attention for the last years. Alas, the HPC community lacks simple yet accurate simulators to predict the energy consumption of general purpose GPU applications. In this work, we address the prediction of the energy consumption of CUDA kernels via simulation. We propose in this paper a simple and lightweight energy model that we implemented using the open-source framework SimGrid. Our proposed model is validated across a diverse set of CUDA kernels and on two different NVIDIA GPUs (Tesla M2075 and Kepler K20Xm). As our modeling approach is not based on performance counters or detailed-architecture parameters, we believe that our model can be easily approved by users who take care of the energy consumption of their GPGPU applications.","2643-3001","978-1-7281-9924-5","10.1109/SBAC-PAD49847.2020.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9235065","GPGPU computing, CUDA kernels, Energy modeling, Simulation","Graphics processing units;Kernel;Energy consumption;Computational modeling;Instruction sets;Computer architecture;Scheduling","application program interfaces;coprocessors;graphics processing units;grid computing;parallel architectures;parallel processing;power aware computing;public domain software","high performance computing;energy consumption challenge;CUDA kernels;lightweight energy model;massively parallel HPC;GPU-based system;SimGrid;NVIDIA GPU;Tesla M2075 GPU;Kepler K20Xm GPU;GPGPU applications;exascale machine","","","","33","","22 Oct 2020","","","IEEE","IEEE Conferences"
"Towards HPC and Big Data Analytics Convergence: Design and Experimental Evaluation of a HPDA Framework for eScience at Scale","D. Elia; S. Fiore; G. Aloisio","Euro-Mediterranean Centre on Climate Change (CMCC) Foundation, Lecce, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Euro-Mediterranean Centre on Climate Change (CMCC) Foundation, Lecce, Italy","IEEE Access","21 May 2021","2021","9","","73307","73326","Over the last two decades, scientific discovery has increasingly been driven by the large availability of data from a multitude of sources, including high-resolution simulations, observations and instruments, as well as an enormous network of sensors and edge components. In such a dynamic and growing landscape where data continue to expand, advances in Science have become intertwined with the capacity of analysis tools to effectively handle and extract valuable information from this ocean of data. In view of the exascale era of supercomputers that is rapidly approaching, it is of the utmost importance to design novel solutions that can take full advantage of the upcoming computing infrastructures. The convergence of High Performance Computing (HPC) and data-intensive analytics is key to delivering scalable High Performance Data Analytics (HPDA) solutions for scientific and engineering applications. The aim of this paper is threefold: reviewing some of the most relevant challenges towards HPDA at scale, presenting a HPDA-enabled version of the Ophidia framework and validating the scalability of the proposed framework through an experimental performance evaluation carried out in the context of the Centre of Excellence in Simulation of Weather and Climate in Europe (ESiWACE). The experimental results show that the proposed solution is capable of scaling over several thousand cores and hundreds of cluster nodes. The proposed work is a contribution in support of scientific large-scale applications along the wider convergence path of HPC and Big Data followed by the scientific research community.","2169-3536","","10.1109/ACCESS.2021.3079139","European Union’s Horizon 2020 Research and Innovation Programme through the Project ESiWACE2(grant numbers:823988); European Union’s Horizon 2020 Research and Innovation Programme through the Project EXDCI-2(grant numbers:800957); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428012","Extreme-scale data challenges;HPC and big data convergence;high performance data analytics (HPDA);performance evaluation;scientific data analysis","Big Data;Data analysis;Convergence;Data models;Europe;Ecosystems;Software","Big Data;data analysis;natural sciences computing;parallel processing;Web services","scientific research community;wider convergence path;large-scale applications;experimental performance evaluation;Ophidia framework;HPDA-enabled version;scientific engineering applications;scalable High Performance Data Analytics solutions;data-intensive analytics;HPC;High Performance Computing;upcoming computing infrastructures;dynamic growing landscape;edge components;high-resolution simulations;scientific discovery;HPDA framework;experimental evaluation","","","","105","CCBY","11 May 2021","","","IEEE","IEEE Journals"
"Parallel Dynamic Data Driven Approaches for Synthetic Aperture Radar","A. Wijayasiri; T. Banerjee; S. Ranka; S. Sahni; M. Schmalz","Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA; Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA; Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA; Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA; Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA","2017 IEEE 24th International Conference on High Performance Computing (HiPC)","8 Feb 2018","2017","","","193","202","Hybrid multicore processors (HMPs) are poised to dominate the landscape of the next generation of computing on the desktop as well as on exascale systems. HMPs consist of general purpose CPU cores along with specialized co-processors and can provide high performance for a wide spectrum of applications at significantly lower energy requirements per FLOP. In this paper, we develop parallel algorithms and software for constructing multi-resolution SAR images on HMPs. We develop several load balancing algorithms for optimizing time performance and energy on HMPs. We also present a systematic approach for deriving the energy-time performance trade-offs on HMPs in the presence of Dynamic Voltage Frequency Scaling. Pareto-optimal curves are presented on a system consisting of 24 traditional cores and a GPU.","","978-1-5386-2293-3","10.1109/HiPC.2017.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8287750","Synthetic Aperture Radar;MultiResolution images;HMP;GPU;Load Balancing;List Assignment;DVFS;Power and Energy Evaluation","Graphics processing units;Synthetic aperture radar;Image reconstruction;Image resolution;Multicore processing;Partitioning algorithms;Load management","coprocessors;multiprocessing systems;parallel algorithms;radar computing;radar imaging;resource allocation;synthetic aperture radar","multiresolution SAR images;HMPs;energy-time performance trade-offs;Dynamic Voltage Frequency Scaling;parallel Dynamic data driven approaches;synthetic aperture radar;hybrid multicore processors;exascale systems;specialized co-processors;parallel algorithms;load balancing algorithms;Pareto-optimal curves;GPU;general purpose CPU cores","","3","","22","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Pre-exascale accelerated application development: The ORNL Summit experience","L. Luo; T. P. Straatsma; L. E. A. Suarez; R. Broer; D. Bykov; E. F. D'Azevedo; S. S. Faraji; K. C. Gottiparthi; C. De Graaf; J. A. Harris; R. W. A. Havenith; H. J. A. Jensen; W. Joubert; R. K. Kathir; J. Larkin; Y. W. Li; D. I. Lyakh; O. E. B. Messer; M. R. Norman; J. C. Oefelein; R. Sankaran; A. F. Tillack; A. L. Barnes; L. Visscher; J. C. Wells; M. Wibowo",NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,"IBM Journal of Research and Development","13 May 2020","2020","64","3/4","11:1","11:21","High-performance computing (HPC) increasingly relies on heterogeneous architectures to achieve higher performance. In the Oak Ridge Leadership Facility (OLCF), Oak Ridge, TN, USA, this trend continues as its latest supercomputer, Summit, entered production in early 2019. The combination of IBM POWER9 CPU and NVIDIA V100 GPU, along with a fast NVLink2 interconnect and other latest technologies, pushes system performance to a new height and breaks the exascale barrier by certain measures. Due to Summit's powerful GPUs and much higher GPU–CPU ratio, offloading to accelerators becomes a requirement for any application, which intends to effectively use the system. To facilitate navigating a complex landscape of competing heterogeneous architectures, a collection of applications from a wide spectrum of scientific domains is selected for early adoption on Summit. In this article, the experience and lessons learned are summarized, in the hope of providing useful guidance to address new programming challenges, such as scalability, performance portability, and software maintainability, for future application development efforts on heterogeneous HPC systems.","0018-8646","","10.1147/JRD.2020.2965881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8960361","","Graphics processing units;Supercomputers;Memory management;Task analysis;Kernel","","","","","","59","IBM","15 Jan 2020","","","IBM","IBM Journals"
"A Community Roadmap for Scientific Workflows Research and Development","R. F. da Silva; H. Casanova; K. Chard; I. Altintas; R. M. Badia; B. Balis; T. Coleman; F. Coppens; F. Di Natale; B. Enders; T. Fahringer; R. Filgueira; G. Fursin; D. Garijo; C. Goble; D. Howell; S. Jha; D. S. Katz; D. Laney; U. Leser; M. Malawski; K. Mehta; L. Pottier; J. Ozik; J. L. Peterson; L. Ramakrishnan; S. Soiland-Reyes; D. Thain; M. Wolf","Oak Ridge National Laboratory,TN,USA; University of Hawaii,Honolulu,HI,USA; The University of Chicago,Chicago,IL,USA; University of California,La Jolla,CA,USA; Barcelona Supercomputing Center,Spain; AGH University of Science and Technology,Krakow,Poland; University of Southern California,Marina Del Rey,CA,USA; Ghent University,Ghent,Belgium; Lawrence Livermore National Lab,Livermore,CA,USA; Lawrence Berkeley National Lab,Berkeley,CA,USA; University of Innsbruck,Innsbruck,Austria; Heriot-Watt University,Edinburgh,UK; OctoML,USA; Universidad Politécnica de Madrid,Spain; The University of Manchester,Manchester,UK; Tweag,Zürich,Switzerland; Brookhaven National Laboratory,Upton,NY,USA; University of Illinois,Urbana-Champaign,USA; Lawrence Livermore National Lab,Livermore,CA,USA; Humboldt-Universität zu Berlin,Berlin,Germany; Oak Ridge National Laboratory,TN,USA; Oak Ridge National Laboratory,TN,USA; University of Southern California,Marina Del Rey,CA,USA; The University of Chicago,Chicago,IL,USA; Lawrence Livermore National Lab,Livermore,CA,USA; Lawrence Livermore National Lab,Livermore,CA,USA; The University of Manchester,Manchester,UK; University of Notre Dame,Indiana,USA; Oak Ridge National Laboratory,TN,USA","2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS)","28 Dec 2021","2021","","","81","90","The landscape of workflow systems for scientific applications is notoriously convoluted with hundreds of seemingly equivalent workflow systems, many isolated research claims, and a steep learning curve. To address some of these challenges and lay the groundwork for transforming workflows research and development, the WorkflowsRI and ExaWorks projects partnered to bring the international workflows community together. This paper reports on discussions and findings from two virtual “Workflows Community Summits” (January and April, 2021). The overarching goals of these workshops were to develop a view of the state of the art, identify crucial research challenges in the workflows community, articulate a vision for potential community efforts, and discuss technical approaches for realizing this vision. To this end, participants identified six broad themes: FAIR computational workflows; AI workflows; exascale challenges; APIs, interoperability, reuse, and standards; training and education; and building a workflows community. We summarize discussions and recommendations for each of these themes.","","978-1-6654-1136-3","10.1109/WORKS54523.2021.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652570","Scientific workflows;community roadmap;data management;AI workflows;exascale computing;interoperability","Training;Conferences;Buildings;Stakeholders;Artificial intelligence;Research and development;Standards","","","","1","","63","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Multiobjective Optimization of SAR Reconstruction on Hybrid Multicore Systems","A. Wijayasiri; T. Banerjee; S. Ranka; S. Sahni; M. Schmalz","Department of Computer, and Information Science, and Engineering, University of Florida, Gainesville, FL, USA; Department of Computer, and Information Science, and Engineering, University of Florida, Gainesville, FL, USA; Department of Computer, and Information Science, and Engineering, University of Florida, Gainesville, FL, USA; Department of Computer, and Information Science, and Engineering, University of Florida, Gainesville, FL, USA; Department of Computer, and Information Science, and Engineering, University of Florida, Gainesville, FL, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Aug 2020","2020","13","","4674","4688","Hybrid multicore processors (HMPs) are poised to dominate the landscape of the next generation of computing on the desktop as well as on exascale systems. HMPs consist of general purpose CPU cores along with specialized coprocessors and can provide high performance for a wide spectrum of applications at significantly lower energy requirements per floating-point operations per second (FLOP). In this article, we develop parallel algorithms and software for constructing multiresolution synthetic aperture radar images on HMPs. We develop several load balancing algorithms for optimizing time performance and energy on HMPs. We also present a systematic approach for deriving the energy-time performance tradeoffs on HMPs in the presence of dynamic voltage frequency scaling. Pareto-optimal curves are presented on a system consisting of 24 traditional cores and a GPU.","2151-1535","","10.1109/JSTARS.2020.3014531","Air Force Office of Scientific Research(grant numbers:FA9550-15-1-0047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159855","Dynamic voltage frequency scaling (DVFS);GPU;hybrid multicore processors (HMP);load balancing;list assignment;multiresolution images;power and energy evaluation;synthetic aperture radar","Graphics processing units;Synthetic aperture radar;Image reconstruction;Image resolution;Multicore processing;Partitioning algorithms;Load management","image reconstruction;multiprocessing systems;optimisation;parallel algorithms;power aware computing;radar imaging;resource allocation;synthetic aperture radar","multiobjective optimization;SAR reconstruction;hybrid multicore systems;hybrid multicore processors;HMPs;exascale systems;floating-point operations;parallel algorithms;multiresolution synthetic aperture radar images;energy-time performance tradeoffs;Pareto-optimal curves;load balancing algorithms","","","","30","CCBY","5 Aug 2020","","","IEEE","IEEE Journals"
"Methodology and Application of HPC: I/O Characterization with MPIProf and IOT","Y. -T. S. Chang; H. Jin; J. Bauer","NASA Adv. Supercomput. Div., NASA Ames Res. Center, Moffett Field, CA, USA; NASA Adv. Supercomput. Div., NASA Ames Res. Center, Moffett Field, CA, USA; I/O Doctors, LLC, Hanska, MN, USA","2016 5th Workshop on Extreme-Scale Programming Tools (ESPT)","26 Jan 2017","2016","","","1","8","Combining the strengths of MPIProf and IOT, an efficient and systematic method is devised for I/O characterization at the per-job, per-rank, per-file and per-call levels of programs running on the high-performance computing resources at the NASA Advanced Supercomputing (NAS) facility. This method is applied to four I/O questions in this paper. A total of 13 MPI programs and 15 cases, ranging from 24 to 5968 ranks, are analyzed to establish the I/O landscape from answers to the four questions. Four of the 13 programs use MPI I/O, and the behavior of their collective writes depends on the specific implementation of the MPI library used. The SGI MPT library, the prevailing MPI library for NAS systems, was found to automatically gather small writes from a large number of ranks in order to perform larger writes by a small subset of collective buffering ranks. The number of collective buffering ranks invoked by MPT depends on the Lustre stripe count and the number of nodes used for the run. A demonstration of varying the stripe count to achieve double-digit speedup of one program's I/O was presented. Another program, which concurrently opens private files by all ranks and could potentially create a heavy load on the Lustre servers, was identified. The ability to systematically characterize I/O for a large number of programs running on a supercomputer, seek I/O optimization opportunity, and identify programs that could cause a high load and instability on the filesystems is important for pursuing exascale in a real production environment.","","978-1-5090-3918-0","10.1109/ESPT.2016.005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830461","MPI I/O;POSIX I/O;Lustre;Stripe count","Internet of Things;Instruments;NASA;Libraries;Optimization;Supercomputers;Bridges","application program interfaces;message passing;parallel processing;software libraries","I/O optimization opportunity;supercomputer;Lustre servers;private files;double-digit speedup;Lustre stripe count;collective buffering ranks;NAS systems;SGI MPT library;MPI library;I/O landscape;MPI programs;I/O questions;NAS facility;NASA advanced supercomputing;high performance computing resources;per-call levels;per-file;per-rank;per-job;IOT;MPIProf;I/O characterization;HPC","","","","17","","26 Jan 2017","","","IEEE","IEEE Conferences"
